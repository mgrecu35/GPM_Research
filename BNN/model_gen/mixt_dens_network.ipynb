{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "#zKuXR=xr.DataArray(np.array(zKuL),dims=['time','range'])\n",
    "#zKu_trueXR=xr.DataArray(np.array(zKu_trueL),dims=['time','range'])\n",
    "#piaKuXR=xr.DataArray(np.array(piaKuL),dims=['time'])\n",
    "#dmXR=xr.DataArray(np.array(dm1DL),dims=['time','range'])\n",
    "#rainRateXR=xr.DataArray(np.array(rainRate1DL),dims=['time','range'])\n",
    "#attKuTopXR=xr.DataArray(np.array(attKuTopL),dims=['time'])\n",
    "#ds=xr.Dataset({'zKu':zKuXR,'zKu_true':zKu_trueXR,'piaKu':piaKuXR,'dm':dmXR,'rainRate':rainRateXR,'attKuTop':attKuTopXR})\n",
    "#compLev=5\n",
    "#encoding={var : {'complevel': compLev} for var in ds.data_vars}\n",
    "#ds.to_netcdf('simulated_radar_rain.nc',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'simulated_radar_rain.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnc\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimulated_radar_rain.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[1;32m      6\u001b[0m     zKu\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzKu\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m      7\u001b[0m     zKu_true\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzKu_true\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'simulated_radar_rain.nc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import netCDF4 as nc\n",
    "with nc.Dataset('simulated_radar_rain.nc') as ds:\n",
    "    zKu=ds['zKu'][:]\n",
    "    zKu_true=ds['zKu_true'][:]\n",
    "    piaKu=ds['piaKu'][:]\n",
    "    dm=ds['dm'][:]\n",
    "    rainRate=ds['rainRate'][:]\n",
    "    attKuTop=ds['attKuTop'][:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, npca=3, nin_channels=32):\n",
    "        super(UNet1D, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(in_channels, nin_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(nin_channels, nin_channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(nin_channels, nin_channels*2, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(nin_channels*2, nin_channels*2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.out_channels = out_channels\n",
    "        self.npca = npca\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv1 = nn.Conv1d(nin_channels*2, nin_channels*2, kernel_size=3, padding=1)\n",
    "        self.bottleneck_conv2 = nn.Conv1d(nin_channels*2, nin_channels*2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv5 = nn.Conv1d(4*nin_channels, 2*nin_channels, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv1d(2*nin_channels, nin_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv7 = nn.Conv1d(2*nin_channels, nin_channels, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv1d(nin_channels, out_channels*(1+npca), kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x1))\n",
    "        x_pool1 = self.pool(x2)\n",
    "        x3 = F.relu(self.conv3(x_pool1))\n",
    "        x4 = F.relu(self.conv4(x3))\n",
    "        x_pool = self.pool(x4)\n",
    "        #print(x_pool.shape)\n",
    "        # Bottleneck\n",
    "        x_bottleneck = F.relu(self.bottleneck_conv1(x_pool))\n",
    "        x_bottleneck = F.relu(self.bottleneck_conv2(x_bottleneck))\n",
    "        # Decoder\n",
    "        x_upsample1 = self.upsample(x_bottleneck)\n",
    "        x_concat = torch.cat([x_upsample1, x3], dim=1)\n",
    "        x5 = F.relu(self.conv5(x_concat))\n",
    "        x6 = F.relu(self.conv6(x5))\n",
    "        up_sample2=self.upsample(x6)\n",
    "        x_concat2 = torch.cat([up_sample2, x2], dim=1)\n",
    "        x7 = F.relu(self.conv7(x_concat2))\n",
    "        x8 = (self.conv8(x7))\n",
    "        x8_mu =  x8[:,:self.out_channels,:]\n",
    "        x8_pca = x8[:,self.out_channels:,:]\n",
    "        return x8_mu, x8_pca\n",
    "# Example usage\n",
    "unet_model = UNet1D(in_channels=1, nin_channels=32, npca=3, out_channels=2)\n",
    "input_tensor = torch.randn(1, 1, 32)\n",
    "output_tensor, output_pca = unet_model(input_tensor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 32])\n",
      "torch.Size([1, 6, 32])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.shape)\n",
    "print(output_pca.shape)\n",
    "nz=32\n",
    "print((output_tensor.view(-1,2*nz)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "pca_v=torch.randn(1,1,3,2*nz)\n",
    "def compute_cov(pca_vectors):\n",
    "    cov = torch.matmul(pca_vectors.transpose(-2,-1), pca_vectors) \n",
    "    return cov\n",
    "\n",
    "cov = compute_cov(pca_v)\n",
    "print(cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(631.7668)\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as D\n",
    "mu=torch.randn(2,1,64)\n",
    "y=torch.randn(2,64)\n",
    "pca_v=torch.randn(2,1,3,64)\n",
    "def pca_mdn_loss(pi, mu, pca_vectors, y):\n",
    "    batch_size, num_mixtures, output_dim = mu.size()\n",
    "    y = y.unsqueeze(1).expand_as(mu)  # shape: [batch_size, num_mixtures, output_dim]\n",
    "    \n",
    "    # Compute covariance matrices from PCA vectors\n",
    "    covariance = compute_cov(pca_vectors)\n",
    "    \n",
    "    # Calculate the log likelihoods for each mixture component\n",
    "    mask = torch.eye(covariance.size(-1), device=covariance.device)  # Identity matrix\n",
    "  # Expand the mask to match the tensor dimensions\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "    covariance = covariance + mask*0.1  # Add a small value to the diagonal for stability\n",
    "    mvn = D.MultivariateNormal(loc=mu, covariance_matrix=covariance)\n",
    "    log_probs = mvn.log_prob(y)\n",
    "    \n",
    "    # Combine log_probs with mixture weights\n",
    "    log_probs = log_probs + torch.log(pi)\n",
    "    \n",
    "    # Log-sum-exp to combine mixture components\n",
    "    log_sum_exp = torch.logsumexp(log_probs, dim=1)\n",
    "    \n",
    "    return -log_sum_exp.mean()\n",
    "pi=torch.ones(2,1)\n",
    "#mvn=D.MultivariateNormal(loc=mu, covariance_matrix=cov)\n",
    "loss=pca_mdn_loss(pi, mu, pca_v, y)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 64, 64])\n",
      "torch.Size([1, 1, 64, 64])\n",
      "(10, 1, 64, 64)\n",
      "[38.78825    66.21794    70.00569     0.10000119  0.09999882  0.09999897\n",
      "  0.10000084  0.09999916  0.0999992   0.10000077  0.09999928  0.1000007\n",
      "  0.10000065  0.09999936  0.10000056  0.09999944  0.09999945  0.10000049\n",
      "  0.09999951  0.10000043  0.09999958  0.1000004   0.09999961  0.09999965\n",
      "  0.09999967  0.09999971  0.10000034  0.10000032  0.10000031  0.09999977\n",
      "  0.10000028  0.10000026  0.09999982  0.10000023  0.10000022  0.10000021\n",
      "  0.09999983  0.09999985  0.10000018  0.09999985  0.10000017  0.09999987\n",
      "  0.09999987  0.10000014  0.0999999   0.0999999   0.10000013  0.10000011\n",
      "  0.1000001   0.1000001   0.10000008  0.10000007  0.09999993  0.09999993\n",
      "  0.10000005  0.09999996  0.09999996  0.09999997  0.10000004  0.10000003\n",
      "  0.09999999  0.1         0.10000002  0.10000002]\n"
     ]
    }
   ],
   "source": [
    "print(cov.shape)\n",
    "pca_v=torch.randn(10,1,3,2*nz)\n",
    "cov = compute_cov(pca_v)\n",
    "mask = torch.eye(cov.size(-1), device=cov.device)\n",
    "mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "cov = cov + mask*0.1\n",
    "print(mask.shape)\n",
    "cov_numpy=cov.detach().numpy()\n",
    "print(cov_numpy.shape)\n",
    "print(np.linalg.eigvals(cov_numpy[0,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.shape)\n",
    "zKum=zKu.mean(axis=0)\n",
    "zKus=zKu.std(axis=0)\n",
    "zKu_scaled=(zKu-zKum)/zKus\n",
    "logRain=np.log(rainRate+1)\n",
    "rainRatem=logRain.mean(axis=0)\n",
    "rainRates=logRain.std(axis=0)\n",
    "rainRate_scaled=(logRain-rainRatem)/rainRates\n",
    "dm_m=dm.mean(axis=0)\n",
    "dm_s=dm.std(axis=0)\n",
    "dm_scaled=(dm-dm_m)/dm_s\n",
    "from sklearn.model_selection import train_test_split\n",
    "y=np.concatenate([rainRate_scaled[:,np.newaxis,:],dm_scaled[:,np.newaxis,:]],axis=1)\n",
    "y=np.concatenate([rainRate_scaled[:,:],dm_scaled[:,:]],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(zKu_scaled, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8375, 64]) 83750 (83750, 1, 32)\n",
      "(83750, 64)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "nt2=X_train.shape[0]//2\n",
    "train_dataset =TensorDataset(torch.tensor(X_train[:nt2,np.newaxis,:][::10],dtype=torch.float32),torch.tensor(y_train[:nt2][::10],dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "print((train_dataset.tensors[1].shape),nt2,X_train[:nt2,np.newaxis,:].shape)\n",
    "print(y_train[:nt2].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2 =TensorDataset(torch.tensor(X_train[nt2:,np.newaxis,:][::10],dtype=torch.float32),torch.tensor(y_train[nt2:][::10],dtype=torch.float32))\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.570751\n",
      "Epoch: 2 \tTraining Loss: 0.469074\n",
      "Epoch: 3 \tTraining Loss: 0.444670\n",
      "Epoch: 4 \tTraining Loss: 0.432290\n",
      "Epoch: 5 \tTraining Loss: 0.421365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unet_model1 = UNet1D(in_channels=1, nin_channels=32, out_channels=2)\n",
    "criterion_mse=nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(unet_model1.parameters(), lr=0.001)\n",
    "n_epochs = 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    ibatch=0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        #print(x_batch.shape,y_batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        y=unet_model1(x_batch)  \n",
    "        loss=criterion_mse(y, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ibatch+=1\n",
    "    train_loss = train_loss / ibatch\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.428620\n",
      "Epoch: 2 \tTraining Loss: 0.386310\n",
      "Epoch: 3 \tTraining Loss: 0.377654\n",
      "Epoch: 4 \tTraining Loss: 0.372665\n",
      "Epoch: 5 \tTraining Loss: 0.369760\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unet_model2 = UNet1D(in_channels=1, nin_channels=32, out_channels=2)\n",
    "criterion_mse=nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(unet_model2.parameters(), lr=0.001)\n",
    "n_epochs = 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = 0.0\n",
    "    ibatch=0\n",
    "    for x_batch, y_batch in train_loader2:\n",
    "        #print(x_batch.shape,y_batch.shape)\n",
    "        optimizer.zero_grad()\n",
    "        y=unet_model2(x_batch)  \n",
    "        loss=criterion_mse(y, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ibatch+=1\n",
    "    train_loss = train_loss / ibatch\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.5668979]\n",
      " [0.5668979 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "yout1=unet_model1(torch.tensor(X_test[:,np.newaxis,:],dtype=torch.float32))\n",
    "print(np.corrcoef(yout1.detach().numpy()[:,0,-1],y_test[:,0,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.91743134]\n",
      " [0.91743134 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#yout2=unet_model2(torch.tensor(X_test[:,np.newaxis,:],dtype=torch.float32))\n",
    "print(np.corrcoef(yout2.detach().numpy()[:,0,-1],yout1.detach().numpy()[:,0,-1]))#y_test[:,0,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000],\n",
      "        [0.5000, 0.0000]])\n",
      "tensor([0., 0.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "a=np.array([[1.0, 1.0], [0.5, 0.0]])\n",
    "a_tensor=torch.tensor(a,dtype=torch.float32)\n",
    "m_tensor=torch.tensor([0.0, 0.0],dtype=torch.float32)\n",
    "print(a_tensor)\n",
    "print(m_tensor)\n",
    "#m = MultivariateNormal(loc=m_tensor, scale_tril=a_tensor)\n",
    "loc = torch.zeros(3)\n",
    "scale = torch.ones(3)\n",
    "print(torch.diag(scale))\n",
    "cholesky=torch.diag(scale)\n",
    "print(cholesky.dtype)\n",
    "cholesky[1,2]=0.5\n",
    "cov_a=torch.mm(cholesky,cholesky.t())\n",
    "print(cholesky.dtype)\n",
    "mvn = MultivariateNormal(loc, covariance_matrix=cov_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril_indices(3, 3, offset=0, dtype=torch.int64, device=None)\n",
    "x1=np.array([1,0])\n",
    "np.dot(x1[:,np.newaxis],x1[:,np.newaxis].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
